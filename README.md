A multimodal system to authenticate users using facial and voice traits with Decision-Level Fusion based on Dempsterâ€“Shafer Evidence Theory.

ğŸ“Œ Overview

This repository contains the implementation of a Multimodal Biometric Authentication System that combines Face Recognition and Voice Recognition to provide secure and reliable identity verification.

The system employs deep learningâ€“based unimodal authentication and integrates their decisions using Decision-Level Fusion based on Dempsterâ€“Shafer (D-S) Evidence Theory, enabling robust authentication under uncertainty and noisy real-world conditions.

This project was developed as part of a Final Year B.Tech Project (2025â€“26) at S. V. National Institute of Technology (SVNIT), Surat.

ğŸ¯ Motivation

Unimodal biometric systems suffer from several limitations, including:

Sensitivity to lighting conditions (Face Recognition)

Background noise and microphone variability (Voice Recognition)

Higher false acceptance and false rejection rates

By combining face and voice modalities, the system improves:

âœ… Accuracy

âœ… Robustness

âœ… Reliability in real-world environments

ğŸ—ï¸ System Architecture

The system consists of four main stages:

1ï¸âƒ£ Data Acquisition

Camera for capturing face images

Microphone for recording voice samples

2ï¸âƒ£ Unimodal Authentication

Face Recognition: FaceNet-based facial embeddings

Voice Recognition: SpeechBrain ECAPA-TDNN speaker embeddings

3ï¸âƒ£ Decision Modeling

Each modality produces an independent authentication decision

4ï¸âƒ£ Decision-Level Fusion

Decisions are fused using Dempsterâ€“Shafer Evidence Theory

A final authentication decision is produced

ğŸ§  Decision-Level Fusion (Dempsterâ€“Shafer Theory)

Each biometric modality provides evidence for:

Genuine

Impostor

Evidence is represented using Basic Probability Assignments (BPAs)

Dempsterâ€“Shafer theory combines evidence while handling uncertainty and conflicts

Provides greater stability and reliability than simple rule-based fusion methods
